<div>
  <article>
    <header>
      <p align="center">
        <img src="img/bme_logo_nagy.jpg" alt="BME Logo" width="30%">
      </p>
      <h1 style="margin-bottom: 20px; text-align: center;">
        Investigations on Speaker Adaptation using a Continuous Vocoder within Recurrent Neural Network based Text-to-Speech Synthesis
      </h1>
      <p align="center">
        <strong>Authors:</strong> Ali Raheem Mandeel, Mohammed Salah Al-Radhi, Tamás Gábor Csapó<br>
        <strong>Email:</strong> {alimandeel, malradhi, csapot}@tmit.bme.hu
      </p>
      <section>
        <h2>Abstract</h2>
        <p>
          This paper presents an investigation of speaker adaptation using a continuous vocoder for parametric text-to-speech (TTS) synthesis. In purposes that demand low computational complexity, conventional vocoder-based statistical parametric speech synthesis can be preferable. While capable of remarkable naturalness, recent neural vocoders nonetheless fall short of the criteria for real-time synthesis. We investigate our former continuous vocoder, in which the excitation is characterized employing two one-dimensional parameters: Maximum Voiced Frequency and continuous fundamental frequency (F0). We show that an average voice can be trained for deep neural network-based TTS utilizing data from nine English speakers. We did speaker adaptation experiments for each target speaker with 400 utterances (approximately 14 minutes). We showed an apparent enhancement in the quality and naturalness of synthesized speech compared to our previous work by utilizing the recurrent neural network topologies. According to the objective studies (Mel-Cepstral Distortion and F0 correlation), the quality of speaker adaptation using Continuous Vocoder-based DNN-TTS is slightly better than the WORLD Vocoder-based baseline. The subjective MUSHRA-like test results also showed that our speaker adaptation technique is almost as natural as the WORLD vocoder using Gated Recurrent Unit and Long Short Term Memory networks. The proposed vocoder, being capable of real-time synthesis, can be used for applications which need fast synthesis speed.
        </p>
      </section>
      <section>
        <h3>Datasets</h3>
        <ul>
          <li><a href="https://datashare.ed.ac.uk/handle/10283/2651" target="_blank">VCTK-Corpus</a></li>
          <li>4 target speakers: 2 Males + 2 Females</li>
        </ul>
      </section>
      <section>
        <h3>Listening Test Results</h3>
        <table border="1" style="width: 100%; margin: 0 auto; text-align: center;">
          <thead>
            <tr>
              <th>Sentence</th>
              <th>Speech (Ground Truth)</th>
              <th>WORLD Vocoder/FFNN</th>
              <th>WORLD Vocoder/GRU</th>
              <th>WORLD Vocoder/LSTM</th>
              <th>Continuous Vocoder/FFNN</th>
              <th>Continuous Vocoder/GRU</th>
              <th>Continuous Vocoder/LSTM</th>
            </tr>
          </thead>
          <tbody>
            <!-- Audio Rows -->
            <script>
              const audioData = [
                { sentence: "F234/01", speaker: "p234", file: "001" },
                { sentence: "F234/02", speaker: "p234", file: "002" },
                { sentence: "F234/03", speaker: "p234", file: "003" },
                { sentence: "F234/04", speaker: "p234", file: "004" },
                { sentence: "M237/01", speaker: "p237", file: "001" },
                // Add more entries as needed
              ];

              function createAudioCell(path) {
                return `<audio controls style="width: 200px;">
                          <source src="${path}" type="audio/wav" />
                          Your browser does not support the audio element.
                        </audio>`;
              }

              audioData.forEach(data => {
                document.write(`<tr>
                  <td>${data.sentence}</td>
                  <td>${createAudioCell(`listening_test_mushra/orginal/${data.speaker}/${data.speaker}_${data.file}.wav`)}</td>
                  <td>${createAudioCell(`listening_test_mushra/world/FFNN/${data.speaker}/test_${data.file}.wav`)}</td>
                  <td>${createAudioCell(`listening_test_mushra/world/GRU/${data.speaker}/test_${data.file}.wav`)}</td>
                  <td>${createAudioCell(`listening_test_mushra/world/LSTM/${data.speaker}/test_${data.file}.wav`)}</td>
                  <td>${createAudioCell(`listening_test_mushra/continuous/FFNN/${data.speaker}/test_${data.file}_synthesized_with_Hilbert.wav`)}</td>
                  <td>${createAudioCell(`listening_test_mushra/continuous/GRU/${data.speaker}/test_${data.file}_synthesized_with_Hilbert.wav`)}</td>
                  <td>${createAudioCell(`listening_test_mushra/continuous/LSTM/${data.speaker}/test_${data.file}_synthesized_with_Hilbert.wav`)}</td>
                </tr>`);
              });
            </script>
          </tbody>
        </table>
      </section>
    </header>
  </article>
</div>
